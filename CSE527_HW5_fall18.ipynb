{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action Recognition @ UCF101  \n",
    "**Due date: 11:59 pm on Dec. 11, 2018 (Tuesday)**\n",
    "\n",
    "## Description\n",
    "---\n",
    "In this homework, you will be doing action recognition using Recurrent Neural Network (RNN), (Long-Short Term Memory) LSTM in particular. You will be given a dataset called UCF101, which consists of 101 different actions/classes and for each action, there will be 145 samples. We tagged each sample into either training or testing. Each sample is supposed to be a short video, but we sampled 25 frames from each videos to reduce the data amount. Consequently, a training sample is a tuple of 3D volume with one dimension encoding *temporal correlation* between frames and a label indicating what action it is.\n",
    "\n",
    "To tackle this problem, we aim to build a neural network that can not only capture spatial information of each frame but also temporal information between frames. Fortunately, you don't have to do this on your own. RNN — a type of neural network designed to deal with time-series data — is right here for you to use. In particular, you will be using LSTM for this task.\n",
    "\n",
    "Instead of training a end-to-end neural network from scratch whose computation is prohibitively expensive for CPUs. We divide this into two steps: feature extraction and modelling. Below are the things you need to implement for this homework:\n",
    "- **{35 pts} Feature extraction**. Use the pretrained VGG network to extract features from each frame. Specifically, we recommend  to use the activations of the first fully connected layer `torchvision.models.vgg16` (4096 dim) as features of each video frame. This will result into a 4096x25 matrix for each video. \n",
    "    **hints**: \n",
    "    - use `scipy.io.savemat()` to save feature to '.mat' file and `scipy.io.loadmat()` load feature.\n",
    "    - norm your images using `torchvision.transforms`\n",
    "    ```\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    prep = transforms.Compose([ transforms.ToTensor(), normalize ])\n",
    "    prep(img)\n",
    "    \n",
    "    ```\n",
    "    More detils of image preprocessing in PyTorch can be found at http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "    \n",
    "- **{35 pts} Modelling**. With the extracted features, build an LSTM network which takes a 4096x25 sample as input, and outputs the action label of that sample.\n",
    "- **{20 pts} Evaluation**. After training your network, you need to evaluate your model with the testing data by computing the prediction accuracy. Moreover, you need to compare the result of your network with that of support vector machine (SVM) (stacking the 4096x25 feature matrix to a long vector and train a SVM).\n",
    "- **{10 pts} Report**. Details regarding the report can be found in the submission section below.\n",
    "\n",
    "Notice that the size of the raw images is 256x340, whereas VGG16 takes 224x224 images as inputs. To solve this problem, instead of resizing the images which unfavorably changes the spatial ratio, we take a better solution: Cropping five 224x224 images at the image center and four corners and compute the 4096-dim VGG16 features for each of them, and average these five 4096-dim feature to get final feature representation for the raw image.\n",
    "\n",
    "In order to save you computational time, we did the feature extraction of most samples for you except for class 1. For class 1, we provide you with the raw images, and you need to write code to extract the feature of the samples in class 1. Instead of training over the whole dataset on CPUs which mays cost you serval days, **use the first 15** classes of the whole dataset. The same applies to those who have access to GPUs.\n",
    "\n",
    "\n",
    "## Dataset\n",
    "Download dataset at [UCF101](http://vision.cs.stonybrook.edu/~yangwang/public/UCF101_dimitris_course.zip). \n",
    "\n",
    "The dataset is consist of the following two parts: video images and extracted features.\n",
    "\n",
    "### 1. Video Images  \n",
    "\n",
    "UCF101 dataset contains 101 actions and 13,320 videos in total.  \n",
    "\n",
    "+ `annos/actions.txt`  \n",
    "  + lists all the actions (`ApplyEyeMakeup`, .., `YoYo`)   \n",
    "  \n",
    "+ `annots/videos_labels_subsets.txt`  \n",
    "  + lists all the videos (`v_000001`, .., `v_013320`)  \n",
    "  + labels (`1`, .., `101`)  \n",
    "  + subsets (`1` for train, `2` for test)  \n",
    "\n",
    "+ `images_class1/`  \n",
    "  + contains videos belonging to class 1 (`ApplyEyeMakeup`)  \n",
    "  + each video folder contains 25 frames  \n",
    "\n",
    "\n",
    "### 2. Video Features\n",
    "\n",
    "+ `extract_vgg16_relu6.py`  \n",
    "  + used to extract video features  \n",
    "     + Given an image (size: 256x340), we get 5 crops (size: 224x224) at the image center and four corners. The `vgg16-relu6` features are extracted for all 5 crops and subsequently averaged to form a single feature vector (size: 4096).  \n",
    "     + Given a video, we process its 25 images seuqentially. In the end, each video is represented as a feature sequence (size: 4096 x 25).  \n",
    "  + written in PyTorch; supports both CPU and GPU.  \n",
    "\n",
    "+ `vgg16_relu6/`  \n",
    "   + contains all the video features, EXCEPT those belonging to class 1 (`ApplyEyeMakeup`)  \n",
    "   + you need to run script `extract_vgg16_relu6.py` to complete the feature extracting process   \n",
    "\n",
    "\n",
    "## Some Tutorials\n",
    "- Good materials for understanding RNN and LSTM\n",
    "    - http://blog.echen.me\n",
    "    - http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "    - http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- Implementing RNN and LSTM with PyTorch\n",
    "    - [LSTM with PyTorch](http://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#sphx-glr-beginner-nlp-sequence-models-tutorial-py)\n",
    "    - [RNN with PyTorch](http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your codes here\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to load the data from text files\n",
    "def loadTextData(path):\n",
    "    data = []\n",
    "    all_data = []\n",
    "    feature = []\n",
    "    label = []\n",
    "    train_test = []\n",
    "    feature_test = []\n",
    "    feature_train = []\n",
    "    label_test = []\n",
    "    label_train = []\n",
    "    with open(path ,'r') as file:\n",
    "        lines = file.read()\n",
    "        line = lines.split(\"\\n\")\n",
    "        #print(line)\n",
    "        #line = shuffle(line)\n",
    "        #print(line)\n",
    "        for i in line:\n",
    "            string = i.split(\"\\t\")\n",
    "            all_data.append(string)\n",
    "    \n",
    "    for i in range(0,2010):\n",
    "        data.append(all_data[i])\n",
    "    shuffle(data)\n",
    "    #print(data)\n",
    "    \n",
    "    for i in range(0, 2010):\n",
    "        feature.append(data[i][0])\n",
    "        label.append(int(data[i][1])-1)\n",
    "        train_test.append(data[i][2])\n",
    "    #print(label)\n",
    "\n",
    "    for i in range(len(train_test)):\n",
    "        a = int(train_test[i])\n",
    "        if a == 1:\n",
    "            feature_train.append(feature[i])\n",
    "            label_train.append(label[i])\n",
    "        if a == 2:\n",
    "            feature_test.append(feature[i])\n",
    "            label_test.append(label[i])\n",
    "    return feature_train, feature_test, label_train, label_test, data\n",
    "\n",
    "path = 'UCF101_dimitris_course/UCF101_release/annos/videos_labels_subsets.txt'\n",
    "feature_train1, feature_test1, label_train1, label_test1, data = loadTextData(path)\n",
    "#print(label_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to get the data in minibatches\n",
    "def getIndexPairs(features, step): \n",
    "    ##Index pairs for mini_batches:\n",
    "    index_pairs = []\n",
    "    indices = []\n",
    "    length = len(features)+1\n",
    "    for i in range(0,length,step):\n",
    "        indices.append(i)\n",
    "    for i in range(len(indices)-1):\n",
    "        current_index = indices[i]\n",
    "        next_index = indices[i+1]\n",
    "        index_pairs.append([current_index, next_index])\n",
    "    return index_pairs\n",
    "step = 7    \n",
    "index_pairs = getIndexPairs(feature_train1, step)\n",
    "index_pairs_test = getIndexPairs(feature_test1, step)\n",
    "##print(index_pairs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 1, 102400])\n"
     ]
    }
   ],
   "source": [
    "##Feeding data in batches to the LSTM network along with corresponding labels as tensors of shape (7,1,102400)\n",
    "def getData(index_pairs, feature_train1, label_train1, path, batch_num):\n",
    "    dim_1 = 25\n",
    "    dim_2 = 4096\n",
    "    channel_num = 1\n",
    "    mini_batch = []\n",
    "    total_data = []\n",
    "    total_label = []\n",
    "    batch_label = []\n",
    "    empty = []\n",
    "    for i in range(len(index_pairs)):\n",
    "        start = index_pairs[i][0]\n",
    "        end = index_pairs[i][1]\n",
    "        mini_batch = copy.deepcopy(empty)\n",
    "        batch_label = copy.deepcopy(empty)\n",
    "        for i in range(start, end):\n",
    "            file_path = feature_train1[i] + '.mat'\n",
    "            file = scipy.io.loadmat(path + feature_train1[i] + '.mat')\n",
    "            mini_batch.append(file['Feature'])\n",
    "            batch_label.append(label_train1[i])\n",
    "            #batch_label = np.array(batch_label)\n",
    "            #print(batch_label)\n",
    "            #print(batch_label.shape)\n",
    "            #batch_label = np.reshape(batch_label,(batch_num,1))\n",
    "            #print(batch_label)\n",
    "        mini_batch = np.array(mini_batch)\n",
    "        mini_batch = torch.from_numpy(np.reshape(mini_batch, (batch_num,channel_num, dim_1*dim_2)))\n",
    "        batch_label = np.array(batch_label)\n",
    "        batch_label = np.reshape(batch_label,(batch_num))\n",
    "        total_data.append(mini_batch)\n",
    "        total_label.append(torch.LongTensor(batch_label))\n",
    "    return total_data, total_label\n",
    "\n",
    "batch_num = 7\n",
    "path =  'UCF101_dimitris_course/UCF101_release/vgg16_relu6/'\n",
    "input_data, label = getData(index_pairs, feature_train1, label_train1, path, batch_num)\n",
    "input_data_test, label_test = getData(index_pairs_test, feature_test1, label_test1, path, batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LSTM network defined using one hidden layer \n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, batch_size, label_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size= batch_size\n",
    "        #self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(102400, hidden_dim)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, input_features):\n",
    "        inputs = input_features.view(self.batch_size,-1)\n",
    "        lstm_out, self.hidden = self.lstm(inputs.view(1,self.batch_size,-1), self.hidden)\n",
    "        outputs = self.hidden2label(lstm_out.view(self.batch_size,-1))\n",
    "        output_labels = F.log_softmax(outputs, dim=-1)\n",
    "        #output = self.softmax(outputs)\n",
    "        return output_labels\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(102400, 256)\n",
      "  (hidden2label): Linear(in_features=256, out_features=15, bias=True)\n",
      ")\n",
      "Epoch : 1 Loss: 0.875\n",
      "Epoch : 2 Loss: 0.094\n",
      "Epoch : 3 Loss: 0.026\n",
      "Epoch : 4 Loss: 0.012\n",
      "Epoch : 5 Loss: 0.008\n",
      "Epoch : 6 Loss: 0.006\n",
      "Epoch : 7 Loss: 0.005\n",
      "Epoch : 8 Loss: 0.004\n",
      "Epoch : 9 Loss: 0.004\n",
      "Epoch : 10 Loss: 0.003\n"
     ]
    }
   ],
   "source": [
    "##Various cases of LSTMs by varying the various hyperparameters mentioned below\n",
    "Batch_size = 7\n",
    "Hidden_dim = 256\n",
    "Num_classes =15\n",
    "\n",
    "model_1 = LSTM(Hidden_dim,Batch_size,Num_classes)\n",
    "print(model_1)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model_1.parameters(), lr=0.1)\n",
    "\n",
    "#Training\n",
    "initial_loss = 0\n",
    "for epoch in range(10):\n",
    "    loss_count = 0.0\n",
    "    for i in range(len(input_data)):\n",
    "        feats = input_data[i]\n",
    "        labels = label[i]\n",
    "        model_1.zero_grad()\n",
    "        model_1.hidden= model_1.init_hidden()\n",
    "        outputs = model_1(feats)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_count += loss.item()\n",
    "    print('Epoch : %d Loss: %.3f' %(epoch+1, loss_count/len(input_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset is: 92.7689594356261\n"
     ]
    }
   ],
   "source": [
    "##Function for testing\n",
    "def Testing(input_data_test, label_test, model):\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(len(input_data_test)):\n",
    "            feats = input_data_test[i]\n",
    "            labels = label_test[i]\n",
    "            outputs = model(feats)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            accuracy = (correct/total)*100\n",
    "    return accuracy\n",
    "\n",
    "accuracy = Testing(input_data_test, label_test, model_1)\n",
    "print(\"Accuracy on the test dataset is:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(102400, 512)\n",
      "  (hidden2label): Linear(in_features=512, out_features=15, bias=True)\n",
      ")\n",
      "Epoch : 1 Loss: 0.634\n",
      "Epoch : 2 Loss: 0.040\n",
      "Epoch : 3 Loss: 0.010\n",
      "Epoch : 4 Loss: 0.005\n",
      "Epoch : 5 Loss: 0.004\n",
      "Epoch : 6 Loss: 0.003\n",
      "Epoch : 7 Loss: 0.002\n",
      "Epoch : 8 Loss: 0.002\n",
      "Epoch : 9 Loss: 0.002\n",
      "Epoch : 10 Loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "Batch_size = 7\n",
    "Hidden_dim = 512\n",
    "Num_classes =15\n",
    "\n",
    "model_2 = LSTM(Hidden_dim,Batch_size,Num_classes)\n",
    "print(model_2)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model_2.parameters(), lr=0.1)\n",
    "\n",
    "#Training\n",
    "initial_loss = 0\n",
    "for epoch in range(10):\n",
    "    loss_count = 0.0\n",
    "    for i in range(len(input_data)):\n",
    "        feats = input_data[i]\n",
    "        labels = label[i]\n",
    "        model_2.zero_grad()\n",
    "        model_2.hidden= model_2.init_hidden()\n",
    "        outputs = model_2(feats)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_count += loss.item()\n",
    "    print('Epoch : %d Loss: %.3f' %(epoch+1, loss_count/len(input_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset is: 92.41622574955908\n"
     ]
    }
   ],
   "source": [
    "accuracy = Testing(input_data_test, label_test, model_2)\n",
    "print(\"Accuracy on the test dataset is:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 1, 102400])\n"
     ]
    }
   ],
   "source": [
    "step = 14  \n",
    "index_pairs = getIndexPairs(feature_train1, step)\n",
    "index_pairs_test = getIndexPairs(feature_test1, step)\n",
    "batch_num = 14\n",
    "path =  'UCF101_dimitris_course/UCF101_release/vgg16_relu6/'\n",
    "input_data, label = getData(index_pairs, feature_train1, label_train1, path, batch_num)\n",
    "input_data_test, label_test = getData(index_pairs_test, feature_test1, label_test1, path, batch_num)\n",
    "print(input_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(102400, 256)\n",
      "  (hidden2label): Linear(in_features=256, out_features=15, bias=True)\n",
      ")\n",
      "Epoch : 1 Loss: 1.062\n",
      "Epoch : 2 Loss: 0.140\n",
      "Epoch : 3 Loss: 0.045\n",
      "Epoch : 4 Loss: 0.020\n",
      "Epoch : 5 Loss: 0.012\n",
      "Epoch : 6 Loss: 0.009\n",
      "Epoch : 7 Loss: 0.007\n",
      "Epoch : 8 Loss: 0.006\n",
      "Epoch : 9 Loss: 0.005\n",
      "Epoch : 10 Loss: 0.004\n"
     ]
    }
   ],
   "source": [
    "Batch_size = 14\n",
    "Hidden_dim = 256\n",
    "Num_classes =15\n",
    "\n",
    "model_1 = LSTM(Hidden_dim,Batch_size,Num_classes)\n",
    "print(model_1)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model_1.parameters(), lr=0.1)\n",
    "\n",
    "#Training\n",
    "initial_loss = 0\n",
    "for epoch in range(10):\n",
    "    loss_count = 0.0\n",
    "    for i in range(len(input_data)):\n",
    "        feats = input_data[i]\n",
    "        labels = label[i]\n",
    "        model_1.zero_grad()\n",
    "        model_1.hidden= model_1.init_hidden()\n",
    "        outputs = model_1(feats)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_count += loss.item()\n",
    "    print('Epoch : %d Loss: %.3f' %(epoch+1, loss_count/len(input_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset is: 92.32142857142858\n"
     ]
    }
   ],
   "source": [
    "accuracy = Testing(input_data_test, label_test, model_1)\n",
    "print(\"Accuracy on the test dataset is:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LSTM with two nn.LSTM layers\n",
    "class LSTM_2(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim_1, hidden_dim_2, batch_size, label_size):\n",
    "        super(LSTM_2, self).__init__()\n",
    "        self.hidden_dim_1 = hidden_dim_1\n",
    "        self.hidden_dim_2 = hidden_dim_2\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        #self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm_1 = nn.LSTM(102400, hidden_dim_1)\n",
    "        self.lstm_2 = nn.LSTM(hidden_dim_1, hidden_dim_2)\n",
    "        \n",
    "        self.hidden2label = nn.Linear(hidden_dim_2, label_size)\n",
    "        \n",
    "        self.hidden1 = self.init_hidden1()\n",
    "        self.hidden2 = self.init_hidden2()\n",
    "    \n",
    "    def init_hidden1(self):\n",
    "        return (torch.zeros(1, 7, self.hidden_dim_1),\n",
    "                torch.zeros(1, 7, self.hidden_dim_1))\n",
    "    def init_hidden2(self):\n",
    "        return(torch.zeros(1, 7, self.hidden_dim_2),\n",
    "               torch.zeros(1, 7, self.hidden_dim_2))\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        inputs = input_features.view(7,-1)\n",
    "        lstm_out_1, self.hidden1 = self.lstm_1(inputs.view(1,7,-1), self.hidden1)\n",
    "        lstm_out_2, self.hidden2 = self.lstm_2(lstm_out_1.view(1,7,-1), self.hidden2)\n",
    "        outputs = self.hidden2label(lstm_out_2.view(7,-1))\n",
    "        output_labels = F.log_softmax(outputs, dim=-1)\n",
    "        return output_labels\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 1, 102400])\n"
     ]
    }
   ],
   "source": [
    "step = 7\n",
    "index_pairs = getIndexPairs(feature_train1, step)\n",
    "index_pairs_test = getIndexPairs(feature_test1, step)\n",
    "batch_num = 7\n",
    "path =  'UCF101_dimitris_course/UCF101_release/vgg16_relu6/'\n",
    "input_data, label = getData(index_pairs, feature_train1, label_train1, path, batch_num)\n",
    "input_data_test, label_test = getData(index_pairs_test, feature_test1, label_test1, path, batch_num)\n",
    "print(input_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_2(\n",
      "  (lstm_1): LSTM(102400, 256)\n",
      "  (lstm_2): LSTM(256, 512)\n",
      "  (hidden2label): Linear(in_features=512, out_features=15, bias=True)\n",
      ")\n",
      "Epoch : 1 Loss: 2.243\n",
      "Epoch : 2 Loss: 0.655\n",
      "Epoch : 3 Loss: 0.160\n",
      "Epoch : 4 Loss: 0.058\n",
      "Epoch : 5 Loss: 0.019\n",
      "Epoch : 6 Loss: 0.009\n",
      "Epoch : 7 Loss: 0.006\n",
      "Epoch : 8 Loss: 0.004\n",
      "Epoch : 9 Loss: 0.003\n",
      "Epoch : 10 Loss: 0.003\n"
     ]
    }
   ],
   "source": [
    "##Model with Hidden dimension size for one layer equal to 1024\n",
    "HIDDEN_DIM_1 = 256\n",
    "HIDDEN_DIM_2 = 512\n",
    "model_3 = LSTM_2(HIDDEN_DIM_1, HIDDEN_DIM_2, 7, 15)\n",
    "print(model_3)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer=optim.SGD(model_3.parameters(), lr=0.1)\n",
    "loss_list = []\n",
    "\n",
    "initial_loss = 0\n",
    "for epoch in range(10):\n",
    "    loss_count = 0.0\n",
    "    for i in range(len(input_data)):\n",
    "        model_3.zero_grad()\n",
    "        model_3.hidden1 = model_3.init_hidden1()\n",
    "        model_3.hidden2 = model_3.init_hidden2()\n",
    "        output_scores = model_3(input_data[i])\n",
    "        loss = loss_function(output_scores, label[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_count += loss.item()\n",
    "    print('Epoch : %d Loss: %.3f' %(epoch+1, loss_count/len(input_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test dataset is: 61.552028218694886\n"
     ]
    }
   ],
   "source": [
    "accuracy = Testing(input_data_test, label_test, model_3)\n",
    "print(\"Accuracy on the test dataset is:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1442\n",
      "568\n"
     ]
    }
   ],
   "source": [
    "###SVM\n",
    "##get data for SVM:\n",
    "def getDataSVM(path, feature_set):\n",
    "    total_data_SVM=[]\n",
    "    dim_1 = 25\n",
    "    dim_2 = 4096\n",
    "    for i in range(len(feature_set)):\n",
    "        file_path = feature_set[i] + '.mat'\n",
    "        file = scipy.io.loadmat(path + file_path)\n",
    "        feature = file['Feature']\n",
    "        feature = np.reshape(feature,(dim_1*dim_2))\n",
    "        total_data_SVM.append(feature)\n",
    "    return total_data_SVM\n",
    "\n",
    "path =  'UCF101_dimitris_course/UCF101_release/vgg16_relu6/'\n",
    "total_data_SVM_train = getDataSVM(path, feature_train1)\n",
    "total_data_SVM_test = getDataSVM(path, feature_test1)\n",
    "    \n",
    "print(len(total_data_SVM_train))\n",
    "print(len(total_data_SVM_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/.local/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/aditya/.local/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LinearSVC(C=0.012, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf =  OneVsRestClassifier(LinearSVC(random_state=None ,tol=1e-4, loss='squared_hinge', C=0.012))\n",
    "clf.fit(total_data_SVM_train, label_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted accuracy is 95.42%\n"
     ]
    }
   ],
   "source": [
    "labels_predicted = clf.predict(total_data_SVM_test)\n",
    "accuracy = accuracy_score(label_test1, labels_predicted)\n",
    "print(\"The predicted accuracy is {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "---\n",
    "**Runnable source code in ipynb file and a pdf report are required**.\n",
    "\n",
    "The report should be of 3 to 4 pages describing what you have done and learned in this homework and report performance of your model. If you have tried multiple methods, please compare your results. If you are using any external code, please cite it in your report. Note that this homework is designed to help you explore and get familiar with the techniques. The final grading will be largely based on your prediction accuracy and the different methods you tried (different architectures and parameters).\n",
    "\n",
    "Please indicate clearly in your report what model you have tried, what techniques you applied to improve the performance and report their accuracies. The report should be concise and include the highlights of your efforts."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
